---
title: "Homework week 8"
author: "Omar Boffil"
date: "7/11/2020"
output: html_document
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm()`, that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `grey`
- `lcol`, for controlling line colors in plots, with a default value of `dodgerblue`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assessing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject."
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals." The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 

Consider using this function to help with the remainder of the assignment as well.
```{r}
diagnostics = function(model, pcol = 'grey', lcol = 'dodgerblue', alpha = 0.05, plotit = TRUE, testit = TRUE) {
par(mfrow = c(1, 2))
  
    if(plotit){
    plot(fitted(model), resid(model), col = pcol, pch = 20, xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
    abline(h = 0, col = lcol, lwd = 2)
    qqnorm(resid(model), main = "Normal Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
    }
  
  if(testit){
    list_elements  = data.frame("p_val" = 0, "decision" = 0);
    list_elements ["p_val"] = shapiro.test(resid(model))$p.value
    
    if(list_elements ["p_val"] > alpha){
      list_elements ["decision"] = "Fail to Reject";
    } else {
      list_elements ["decision"] = "Reject";
    }
    
    list_elements 
  } 
  

  
}
```


**(b)** Run the following code.

```{r}
set.seed(40)

data_1 = data.frame(x = runif(n = 30, min = 0, max = 10),
                    y = rep(x = 0, times = 30))
data_1$y = with(data_1, 2 + 1 * x + rexp(n = 30))
fit_1 = lm(y ~ x, data = data_1)

data_2 = data.frame(x = runif(n = 20, min = 0, max = 10),
                    y = rep(x = 0, times = 20))
data_2$y = with(data_2, 5 + 2 * x + rnorm(n = 20))
fit_2 = lm(y ~ x, data = data_2)

data_3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                    y = rep(x = 0, times = 40))
data_3$y = with(data_3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit_3 = lm(y ~ x, data = data_3)
```

```{r fig.height=5, fig.width=10}
diagnostics(fit_1, plotit = FALSE)$p_val
diagnostics(fit_2, plotit = FALSE)$decision
diagnostics(fit_1, testit = FALSE, pcol = "black", lcol = "black")
diagnostics(fit_2, testit = FALSE, pcol = "grey", lcol = "green")
diagnostics(fit_3)
```

***

## Exercise 2 (Prostate Cancer Data)

For this exercise, we will use the `prostate` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?prostate` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit an additive multiple regression model with `lpsa` as the response and the remaining variables in the `prostate` dataset as predictors. Report the $R^2$ value for this model.

```{r}
add_model = lm(lpsa ~ ., data = prostate)
summary(add_model)$r.squared
```
**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.
```{r fig.height=5, fig.width=10}
diagnostics(add_model, testit = FALSE)
```
```{r}
library(lmtest)
bptest(add_model)
```
The Fitted vs Residual plot looks like it has a constant variance of the error and we can see that the studentized Breusch-Pagan test does not reject, so we can conclude that the variance has not been violated

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.

```{r}
diagnostics(add_model, plotit = FALSE)$p_val
diagnostics(add_model, plotit = FALSE)$decision
```
The Q-Q Plot above have values close to zero but for me is not enough, so I checked the Shapiro-Wilk test and it fails to Reject with means that and I high p-value which means it has not been violated

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.
```{r}
high_leverage = as.vector(which(hatvalues(add_model) > 2 * mean(hatvalues(add_model))))
prostate[high_leverage,]
```

**(e)** Check for any influential observations. Report any observations you determine to be influential.
```{r}
influential = as.vector(which(cooks.distance(add_model) > 4 / length(cooks.distance(add_model))))
prostate[influential,]
```



**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

```{r}
not_influential = prostate[-influential,]
add_model_not_influential = lm(lpsa ~ ., data = not_influential)
data.frame("Original" = coef(add_model), "modified" = coef(add_model_not_influential))
```


**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

```{r}
df_removed = prostate[influential,]
predict(add_model, newdata = df_removed) - predict(add_model_not_influential, newdata = df_removed)

```
The prediction in the models is no equal, which could mean that they have some influence on the model.
***

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameter esimators that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 5)
x_2 = runif(n, -2, 2)
```

Consider the model,

\[
Y = 4 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 4
- $\beta_1$ = 1
- $\beta_2$ = 0

We now simulate `y_1` in a manner that does **not** violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(83)
library(lmtest)
y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
fit_1 = lm(y_1 ~ x_1 + x_2)
bptest(fit_1)
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
set.seed(83)
y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
fit_2 = lm(y_2 ~ x_1 + x_2)
bptest(fit_2)
```

**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 2500
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
birthday = 19900826
set.seed(birthday)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)

```{r}
for(i in 1:num_sims){
  
  y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
  fit_1 = lm(y_1 ~ x_1 + x_2)
  p_val_1[i] = bptest(fit_1)$p.value
  
  y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
  fit_2 = lm(y_2 ~ x_1 + x_2)
  p_val_2[i] = bptest(fit_2)$p.value
  
}
```


**(b)** What proportion of the `p_val_1` values is less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values is less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

```{r}

library(knitr)

p_value_results = data.frame("P_value_proportion_y_1" = c("0.01" = mean(p_val_1 < 0.01), "0.05" = mean(p_val_1 < 0.05),"0.10" = mean(p_val_1 < 0.10)
  ),"P_value_proportion_y_2" = c("0.01" = mean(p_val_2 < 0.01), "0.05" = mean(p_val_2 < 0.05), "0.10" = mean(p_val_2 < 0.10)))

p_value_results

```
We were expecting that if both models did not violate any assumptions, the proportion of p-values for each $\alpha$ should be less than the corresponding $\alpha$. but In the case of fit_2, the assumptions violated it for $\alpha$ 0.05 and 0.10 $\sigma$, and this could be for the value of $\sigma$ for Y_2

***

## Exercise 4 (Corrosion Data)

For this exercise, we will use the `corrosion` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?corrosion` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `loss` as the response and `Fe` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.


```{r}
corr_model = lm(loss ~ Fe, data = corrosion)
plot(loss ~ Fe, data = corrosion, main = "Weight loss vs Iron Content", col = "Red")
abline(corr_model, col = "blue")
```


```{r fig.height=5, fig.width=10}
par(mfrow = c(1, 2))


plot(fitted(corr_model), resid(corr_model), col = "grey", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals")
abline(h = 0, col = "dodgerblue", lwd = 2)

qqnorm(resid(corr_model), main = "Normal Q-Q Plot", col = "darkgrey")
qqline(resid(corr_model), col = "dodgerblue", lwd = 2)

```

```{r}
# Test for non-constant variance
bp_pvalue = bptest(corr_model)$p.value;
bp_pvalue

# Test for noraml distribution
shapiro_pvalue = shapiro.test(resid(corr_model))$p.value
shapiro_pvalue
```

The fitted vs residual plot looks good, but the Q-Q Plot has some values at the end that looks suspicious, but after the results of the studentized Breusch-Pagan test and Shapiro test to test for non-constant variance (`r bp_pvalue`) and normal distribution (`r shapiro_pvalue`) both test fail to reject the null hypothesis which means we can process with the models

**(b)** Fit higher order polynomial models of degree 2, 3, and 4. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.


```{r fig.height=5, fig.width=10}
model_p_2 = lm(loss ~ Fe + I(Fe^2), data = corrosion)
model_p_3 = lm(loss ~ Fe + I(Fe^2) + I(Fe^3), data = corrosion)
model_p_4 = lm(loss ~ Fe + I(Fe^2) + I(Fe^3) + I(Fe^4), data = corrosion)


par(mfrow = c(1, 3))

plot(fitted(model_p_2), resid(model_p_2), col = "red", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals degree 2")
abline(h = 0, col = "blue", lwd = 2);


plot(fitted(model_p_3), resid(model_p_3), col = "red", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals degree 3")
abline(h = 0, col = "blue", lwd = 2);


plot(fitted(model_p_4), resid(model_p_4), col = "red", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residuals degree 4")
abline(h = 0, col = "blue", lwd = 2);
```
Base on the plots, I will choose the one with the degree 3 because the variance looks equivalent in both sides of the mean.

```{r}
p_value_results = data.frame("P_value_variance" = c("Degree2" = bptest(model_p_2)$p.value, "Degree3" = bptest(model_p_3)$p.value,"Degree4" = bptest(model_p_4)$p.value)
                             
,"P_value_Shapiro" = c("Degree2" = shapiro.test(resid(model_p_2))$p.value, "Degree3" = shapiro.test(resid(model_p_3))$p.value, "Degree4" = shapiro.test(resid(model_p_4))$p.value))

p_value_results
```

After comparing them, we will select the models with degrees three that fail to reject both values and has a small error of variance and the Highest P-value.

```{r}

cooks.distance(model_p_3) > 4 / length(cooks.distance(model_p_3))
```
After checking for the cook distance test, we found out that it does not have any influential observations 

***

## Exercise 5 (Diamonds)

The data set `diamonds` from the `ggplot2` package contains prices and characteristics of 54,000 diamonds. For this exercise, use `price` as the response variable $y$, and `carat` as the predictor $x$. Use `?diamonds` to learn more.

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
```

**(a)** Fit a linear model with `price` as the response variable $y$, and `carat` as the predictor $x$. Return the summary information of this model.

```{r}
model_dimonds = lm(price ~ carat, data = diamonds)
summary(model_dimonds)
```

**(b)** Plot a scatterplot of price versus carat and add the line for the fitted model in part **(a)**. Using a fitted versus residuals plot and/or a Q-Q plot, comment on the diagnostics. 

```{r}
plot(price ~ carat, data = diamonds, main = "Price vs Carat", col = "grey", pch = 20, cex = 1.5)
abline(model_dimonds, col = "dodgerblue", lwd = 2)
```

```{r fig.height=5, fig.width=10}
diagnostics(model_dimonds, testit = FALSE)
```
It is a clear violation of the three plot, and what they represent, the fitted vs. residual plot shows the bad distribution in the variance, and Q-Q plot has big tails that show the that miss the normality of error. 

**(c)** Seeing as the price stretches over several orders of magnitude, it seems reasonable to try a log transformation of the response. Fit a model with a logged response, plot a scatterplot of log-price versus carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
model_log = lm(log(price) ~ carat, data = diamonds)
plot(log(price) ~ carat, data = diamonds, main = "Price vs Carat", col = "grey", pch = 20, cex = 1.5)
abline(model_log, col = "dodgerblue", lwd = 2)
```
```{r fig.height=5, fig.width=10}
diagnostics(model_log, testit = FALSE)
```

The log of the response did not have the expected effect. Still, three plot showing the same problems, the Fitted vs Residual plot show that the variance is no well distributed from the zero mean and the Q-Q plot get better but no enough because it still has a big tail in the chart


```{r}
qplot(price, data = diamonds, bins = 30)

```


**(d)** Try adding log transformation of the predictor. Fit a model with a logged response and logged predictor, plot a scatterplot of log-price versus log-carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
model_log_log = lm(log(price) ~ log(carat), data = diamonds)
plot(log(price) ~ log(carat), data = diamonds, main = "Price vs Carat", col = "grey", pch = 20, cex = 1.5)
abline(model_log_log, col = "dodgerblue", lwd = 2)
```

```{r fig.height=5, fig.width=10}
diagnostics(model_log_log, testit = FALSE)
```

The regression line plot now looks good, the Fitted vs Residual plot look way better than before, and it is possible that we can find equality in the variance, and the Q-Q improved the normality but still have tails, so we could do some test in the model to see if it violated or not the assumption.  


**(e)** Use the model from part **(d)** to predict the price (in dollars) of a 3-carat diamond. Construct a 99% prediction interval for the price (in dollars).

```{r}
price_pred = exp(predict(model_log_log, newdata = data.frame("carat" = 3), interval = "prediction", level = 0.99 ))
round(price_pred[1,1], 2)
```
The average prince for a 3 carat diamond is `r round(price_pred[1,1], 2)` dollars with boundaries between `r round(price_pred[1,2], 2)` and `r round(price_pred[1,3], 2)` dollars


